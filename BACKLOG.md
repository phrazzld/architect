# BACKLOG

- [~] **T13: Add synthesis step**
  Add a built-in synthesis step where outputs from multiple preceding steps (e.g., multiple model responses, critiques) are sent to a final model for summarization / consolidation. i suspect implementation is some optional --synthesis-model flag where the user can set the model to use for synthesis

- [ ] **T14: Implement semantic versioning**
  Implement semantic versioning (ideally automatically managed somehow ... conventional commits?)

- [ ] **T11: Audit codebase against dev philosophy**
  Audit whole codebase against dev philosophy, identify key things to hit

- [ ] add grok support for native grok api

- [ ] **T12: Code minimization**
  Remove as much as possible, shrink the codebase

- [ ] **T15: Improve arbitrary model handling**
  Improve arbitrary model handling

- [ ] **T16: Add Gemini grounding support**
  Add support for grounding with gemini models

- [ ] **T17: Refactor output handling**
  Refactor output handling to use standard streams (stdout for primary results, stderr for logs/errors) and add a json output mode flag (`--output-format json`) for machine-readable results.

- [ ] **T18: Use distinct exit codes**
  Use distinct exit codes for different outcomes (success, user error, api error, file system error) to allow programmatic result checking.

- [ ] **T19: Make file output optional**
  Make writing plan output to a file optional (`--output <path>`) and default to printing the plan to stdout if the flag is omitted

- [ ] **T20: Allow custom system prompts**
  Allow users to define and pass custom system prompts to the models.

- [ ] **T21: Enable custom model parameters**
  Enable users to provide full model configuration parameters (temperature, top-p, max output tokens, etc.) via cli flags or a configuration file

- [ ] **T22: Cache model metadata**
  Fetch, cache, and keep updated model metadata (e.g., max tokens, cost per token, input/output limits, usage tips) from provider apis or documentation for all supported models

- [ ] **T23: Implement token counting**
  Implement robust token counting using provider apis, display usage relative to model limits, warn if limits are approached, and potentially halt execution if limits are exceeded

- [ ] **T24: Estimate request costs**
  Estimate request cost based on token count and model pricing information, log the estimated cost, and potentially integrate with provider billing apis for accuracy.

- [ ] **T25: Develop workflow engine**
  Develop a flexible multi-step workflow engine allowing users to define arbitrary sequences of operations (e.g., plan -> critique -> revise -> synthesize) via configuration or cli flags.
  * this might just be achieved by designing the program to support piping, ie take the multiple responses generated by the first pass and pipe them in as context for another pass

- [ ] **T26: Support multiple concurrent models**
  Support querying multiple models concurrently for the same task (`--model model_a,model_b`) to get a "council of experts" perspective, returning labeled outputs.

- [ ] **T27: Implement built-in workflows**
  Implement a built-in plan -> critique -> refine workflow activated by a specific flag (e.g., `--critique-refine`).

- [ ] **T28: Add context preprocessing**
  Add an optional context preprocessing step to summarize large context inputs before sending them to the llm, potentially triggered automatically for models with smaller context windows.

- [ ] **T29: Auto-select appropriate models**
  Explore automatically selecting the most appropriate model or models based on the task description and context size/type.

- [ ] **T30: Integrate AST parsing**
  Integrate abstract syntax tree (ast) parsing for supported languages to provide richer structural context to the llm beyond raw text.

- [ ] **T31: Add git integration**
  Add git integration to use `git diff` output as context (`--context-git-diff <ref>`) or optionally include `git blame` / commit history for context files.

- [ ] **T32: Investigate language server integration**
  Investigate integrating with language servers or symbol indexing tools (ctags, sourcegraph api) to provide symbol definition/usage information as context.

- [ ] **T33: Support structured output formats**
  Support generating output plans in structured formats like json or yaml in addition to markdown.

- [ ] **T34: Add code generation mode**
  Add a code generation mode to attempt generating code snippets or files based on the generated plan.

- [ ] **T35: Provide patch file output**
  Provide an option to output suggested code changes (especially for refactoring) as a `.patch` file.

- [ ] **T36: Support Claude memory files**
  Make `thinktank` aware of claude code's memory files (`CLAUDE.md`, `CLAUDE.local.md`) to read configuration settings, respecting the same hierarchy

- [ ] **T37: Integrate with Claude Code**
  Investigate presenting `thinktank` itself as a tool to claude code, potentially via mcp, defining its capabilities for planning, critique, and refinement

- [ ] **T38: Improve error messages**
  Review and significantly improve the clarity, detail, and actionability of all error messages throughout the application

- [ ] **T39: Add context metadata**
  Add metadata (file paths, git status) to the context provided to the llm.

- [ ] **T40: Add instruction enhancement**
  Support a "modify my instructions" flag that extracts intent from the passed instructions and sends it to a model to rewrite according to best prompt engineering practices before sending your actual request complete with context to your target models
## OpenAI Integration Improvements

- [ ] **T42: Consolidate API Services**
  Deprecate and remove the legacy `apiService` implementation to avoid confusion and inconsistent behavior between the legacy and registry implementations.

- [ ] **T43: Improve Model Name Lookup**
  Make model lookup in the registry case-insensitive to prevent user confusion with model capitalization.

- [ ] **T44: Standardize API Key Handling**
  Ensure consistent API key resolution logic across all code paths and providers to prevent subtle bugs.

- [ ] **T45: Restore Integration Tests**
  Update and re-enable the disabled integration tests to verify end-to-end functionality across all providers.
