# backlog

* rebrand to thinktank

* remove the cutesy directory naming stuff -- simplify

* audit whole codebase against dev philosophy, identify key things to hit
* remove as much as possible, shrink it
* add a built-in synthesis step where outputs from multiple preceding steps (e.g., multiple model responses, critiques) are sent to a final model for summarization or consolidation.
* implement semantic versioning (ideally automatically managed somehow ... conventional commits?)
* improve arbitrary model handling
* add support for grounding with gemini models
* refactor output handling to use standard streams (stdout for primary results, stderr for logs/errors) and add a json output mode flag (`--output-format json`) for machine-readable results.
* use distinct exit codes for different outcomes (success, user error, api error, file system error) to allow programmatic result checking.
* make writing plan output to a file optional (`--output <path>`) and default to printing the plan to stdout if the flag is omitted
* allow users to define and pass custom system prompts to the models.
* enable users to provide full model configuration parameters (temperature, top-p, max output tokens, etc.) via cli flags or a configuration file
* fetch, cache, and keep updated model metadata (e.g., max tokens, cost per token, input/output limits, usage tips) from provider apis or documentation for all supported models
* implement robust token counting using provider apis, display usage relative to model limits, warn if limits are approached, and potentially halt execution if limits are exceeded
* estimate request cost based on token count and model pricing information, log the estimated cost, and potentially integrate with provider billing apis for accuracy.
* develop a flexible multi-step workflow engine allowing users to define arbitrary sequences of operations (e.g., plan -> critique -> revise -> synthesize) via configuration or cli flags.
  * this might just be achieved by designing the program to support piping, ie take the multiple responses generated by the first pass and pipe them in as context for another pass
* support querying multiple models concurrently for the same task (`--model model_a,model_b`) to get a "council of experts" perspective, returning labeled outputs.
* implement a built-in plan -> critique -> refine workflow activated by a specific flag (e.g., `--critique-refine`).
* add an optional context preprocessing step to summarize large context inputs before sending them to the llm, potentially triggered automatically for models with smaller context windows.
* explore automatically selecting the most appropriate model or models based on the task description and context size/type.
* integrate abstract syntax tree (ast) parsing for supported languages to provide richer structural context to the llm beyond raw text.
* add git integration to use `git diff` output as context (`--context-git-diff <ref>`) or optionally include `git blame` / commit history for context files.
* investigate integrating with language servers or symbol indexing tools (ctags, sourcegraph api) to provide symbol definition/usage information as context.
* support generating output plans in structured formats like json or yaml in addition to markdown.
* add a code generation mode to attempt generating code snippets or files based on the generated plan.
* provide an option to output suggested code changes (especially for refactoring) as a `.patch` file.
* make `thinktank` aware of claude code's memory files (`CLAUDE.md`, `CLAUDE.local.md`) to read configuration settings, respecting the same hierarchy
* investigate presenting `thinktank` itself as a tool to claude code, potentially via mcp, defining its capabilities for planning, critique, and refinement
* review and significantly improve the clarity, detail, and actionability of all error messages throughout the application
* add metadata (file paths, git status) to the context provided to the llm.
* support a "modify my instructions" flag that extracts intent from the passed instructions and sends it to a model to rewrite according to best prompt engineering practices before sending your actual request complete with context to your target models
