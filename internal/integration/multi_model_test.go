// internal/integration/multi_model_test.go
package integration

import (
	"context"
	"fmt"
	"os"
	"path/filepath"
	"strings"
	"sync"
	"testing"
	"time"

	"github.com/phrazzld/architect/internal/architect"
	"github.com/phrazzld/architect/internal/gemini"
	"github.com/phrazzld/architect/internal/logutil"
)

// TestMultiModelExecution tests the execution with multiple models
func TestMultiModelExecution(t *testing.T) {
	// Set up the test environment
	env := NewTestEnv(t)
	defer env.Cleanup()

	// Save model names that were processed
	processedModels := make(map[string]bool)

	// Set up a custom mock client behavior to track model names
	env.MockClient.GenerateContentFunc = func(ctx context.Context, prompt string) (*gemini.GenerationResult, error) {
		// Extract the model name from the API service's InitClient call
		// This relies on the fact that we're setting a model-specific string in the result
		// to identify which model generated which content
		modelName := ctx.Value(modelNameKey).(string)
		processedModels[modelName] = true

		return &gemini.GenerationResult{
			Content:      "# Plan Generated by " + modelName + "\n\nThis is a test plan generated for the model.",
			TokenCount:   1000,
			FinishReason: "STOP",
		}, nil
	}

	// Create a test file
	env.CreateTestFile(t, "src/main.go", `package main

func main() {}`)

	// Create an instructions file
	instructionsFile := env.CreateTestFile(t, "instructions.md", "Test multi-model generation")

	// Set up the output directory
	outputDir := filepath.Join(env.TestDir, "output")

	// Create a test configuration with multiple models
	modelNames := []string{"model1", "model2", "model3"}
	testConfig := &architect.CliConfig{
		InstructionsFile: instructionsFile,
		OutputDir:        outputDir,
		ModelNames:       modelNames,
		ApiKey:           "test-api-key",
		Paths:            []string{env.TestDir + "/src"},
		LogLevel:         logutil.InfoLevel,
	}

	// Create a custom mock API service that can track model names
	mockAPIService := &mockModelTrackingAPIService{
		logger:     env.Logger,
		mockClient: env.MockClient,
	}

	// Run the application directly with our custom mock API service
	ctx := context.Background()
	err := architect.RunInternal(
		ctx,
		testConfig,
		env.Logger,
		mockAPIService,
		env.AuditLogger,
	)

	if err != nil {
		t.Fatalf("RunInternal failed: %v", err)
	}

	// Check that the output directory exists
	if _, err := os.Stat(outputDir); os.IsNotExist(err) {
		t.Errorf("Output directory was not created at %s", outputDir)
	}

	// Check that we have a separate output file for each model
	for _, modelName := range modelNames {
		// Construct the expected file path (as the implementation would)
		outputFile := filepath.Join(outputDir, modelName+".md")

		// Check that the file exists
		if _, err := os.Stat(outputFile); os.IsNotExist(err) {
			t.Errorf("Output file for model %s was not created at %s", modelName, outputFile)
			continue
		}

		// Read the content to verify it contains model-specific content
		content, err := os.ReadFile(outputFile)
		if err != nil {
			t.Errorf("Failed to read output file for model %s: %v", modelName, err)
			continue
		}

		// Verify the content contains the model name
		if !strings.Contains(string(content), "Plan Generated by "+modelName) {
			t.Errorf("Output file for model %s does not contain model-specific content. Content: %s",
				modelName, string(content))
		}
	}

	// Verify that all models were processed
	for _, modelName := range modelNames {
		if !processedModels[modelName] {
			t.Errorf("Model %s was not processed", modelName)
		}
	}
}

// Define a type for the context key to avoid string collisions
type contextKey string

// Define a constant for the model name key
const modelNameKey contextKey = "current_model"

// mockModelTrackingAPIService extends mockIntAPIService to track model names
type mockModelTrackingAPIService struct {
	logger     logutil.LoggerInterface
	mockClient gemini.Client
}

// InitClient returns the mock client and stores model name in context
func (s *mockModelTrackingAPIService) InitClient(ctx context.Context, apiKey, modelName string) (gemini.Client, error) {
	// Create a new context with the model name
	ctx = context.WithValue(ctx, modelNameKey, modelName)

	// Set the context in the mock client
	mockClient := &modelAwareClient{
		delegateClient: s.mockClient,
		ctx:            ctx,
	}

	return mockClient, nil
}

// Process responses the same as mockIntAPIService
func (s *mockModelTrackingAPIService) ProcessResponse(result *gemini.GenerationResult) (string, error) {
	if result == nil {
		return "", fmt.Errorf("empty response from API")
	}
	if result.Content == "" {
		return "", fmt.Errorf("empty content from API")
	}
	return result.Content, nil
}

func (s *mockModelTrackingAPIService) IsEmptyResponseError(err error) bool {
	if err == nil {
		return false
	}
	return strings.Contains(err.Error(), "empty")
}

func (s *mockModelTrackingAPIService) IsSafetyBlockedError(err error) bool {
	if err == nil {
		return false
	}
	return strings.Contains(err.Error(), "safety")
}

func (s *mockModelTrackingAPIService) GetErrorDetails(err error) string {
	if err == nil {
		return ""
	}
	return err.Error()
}

// modelAwareClient wraps a Client to preserve the context with model name
type modelAwareClient struct {
	delegateClient gemini.Client
	ctx            context.Context
}

func (c *modelAwareClient) GenerateContent(ctx context.Context, prompt string) (*gemini.GenerationResult, error) {
	// Use the stored context that has the model name instead of the provided one
	return c.delegateClient.GenerateContent(c.ctx, prompt)
}

func (c *modelAwareClient) CountTokens(ctx context.Context, prompt string) (*gemini.TokenCount, error) {
	return c.delegateClient.CountTokens(ctx, prompt)
}

func (c *modelAwareClient) GetModelInfo(ctx context.Context) (*gemini.ModelInfo, error) {
	return c.delegateClient.GetModelInfo(ctx)
}

func (c *modelAwareClient) Close() error {
	return c.delegateClient.Close()
}

// TestModelFailureHandling tests the behavior when one model fails but others succeed
func TestModelFailureHandling(t *testing.T) {
	// Set up the test environment
	env := NewTestEnv(t)
	defer env.Cleanup()

	// Track model processing attempts
	processedModels := make(map[string]bool)
	failedModels := make(map[string]bool)

	// Set up a custom mock client behavior to make the second model fail
	env.MockClient.GenerateContentFunc = func(ctx context.Context, prompt string) (*gemini.GenerationResult, error) {
		// Extract the model name
		modelName := ctx.Value(modelNameKey).(string)
		processedModels[modelName] = true

		// Make model2 fail
		if modelName == "model2" {
			failedModels[modelName] = true
			return nil, fmt.Errorf("simulated error for model %s", modelName)
		}

		return &gemini.GenerationResult{
			Content:      "# Plan Generated by " + modelName + "\n\nThis is a test plan generated for the model.",
			TokenCount:   1000,
			FinishReason: "STOP",
		}, nil
	}

	// Create a test file
	env.CreateTestFile(t, "src/main.go", `package main

func main() {}`)

	// Create an instructions file
	instructionsFile := env.CreateTestFile(t, "instructions.md", "Test multi-model generation with error handling")

	// Set up the output directory
	outputDir := filepath.Join(env.TestDir, "output")

	// Create a test configuration with multiple models
	modelNames := []string{"model1", "model2", "model3"}
	testConfig := &architect.CliConfig{
		InstructionsFile: instructionsFile,
		OutputDir:        outputDir,
		ModelNames:       modelNames,
		ApiKey:           "test-api-key",
		Paths:            []string{env.TestDir + "/src"},
		LogLevel:         logutil.InfoLevel,
	}

	// Create a custom mock API service that can track model names
	mockAPIService := &mockModelTrackingAPIService{
		logger:     env.Logger,
		mockClient: env.MockClient,
	}

	// Run the application directly with our custom mock API service
	ctx := context.Background()
	err := architect.RunInternal(
		ctx,
		testConfig,
		env.Logger,
		mockAPIService,
		env.AuditLogger,
	)

	// Should have an error because model2 failed
	if err == nil {
		t.Fatalf("Expected RunInternal to fail due to model2 error, but it succeeded")
	}

	// Error should mention model2
	if !strings.Contains(err.Error(), "model2") {
		t.Errorf("Error message does not mention failed model: %v", err)
	}

	// Check that the output directory exists
	if _, err := os.Stat(outputDir); os.IsNotExist(err) {
		t.Errorf("Output directory was not created at %s", outputDir)
	}

	// Check output files for successful models
	for _, modelName := range []string{"model1", "model3"} {
		outputFile := filepath.Join(outputDir, modelName+".md")

		// Check that the file exists
		if _, err := os.Stat(outputFile); os.IsNotExist(err) {
			t.Errorf("Output file for successful model %s was not created at %s", modelName, outputFile)
			continue
		}

		// Read the content to verify it contains model-specific content
		content, err := os.ReadFile(outputFile)
		if err != nil {
			t.Errorf("Failed to read output file for model %s: %v", modelName, err)
			continue
		}

		// Verify the content contains the model name
		if !strings.Contains(string(content), "Plan Generated by "+modelName) {
			t.Errorf("Output file for model %s does not contain model-specific content", modelName)
		}
	}

	// Check that the failed model doesn't have an output file
	failedOutputFile := filepath.Join(outputDir, "model2.md")
	if _, err := os.Stat(failedOutputFile); !os.IsNotExist(err) {
		t.Errorf("Output file for failed model was created at %s (it shouldn't exist)", failedOutputFile)
	}

	// Verify that all models were attempted
	for _, modelName := range modelNames {
		if !processedModels[modelName] {
			t.Errorf("Model %s was not attempted", modelName)
		}
	}

	// Verify that only model2 failed
	if !failedModels["model2"] {
		t.Errorf("Expected model2 to fail, but it didn't")
	}
	if len(failedModels) != 1 {
		t.Errorf("Expected exactly one model to fail, but got %d failed models", len(failedModels))
	}
}

// TestConcurrentModelProcessing tests that models are processed concurrently
// by checking that their processing overlaps in time
func TestConcurrentModelProcessing(t *testing.T) {
	// Set up the test environment
	env := NewTestEnv(t)
	defer env.Cleanup()

	// Use synchronization primitives to track concurrent execution
	var processingStarted sync.WaitGroup
	var processingBarrier sync.WaitGroup
	var processingCompleted sync.WaitGroup

	// Set up the barriers
	modelCount := 3
	processingStarted.Add(modelCount)
	processingBarrier.Add(1) // Single barrier to hold all goroutines until all have started
	processingCompleted.Add(modelCount)

	// Track execution data
	processOrder := &struct {
		sync.Mutex
		started   []string
		completed []string
	}{
		started:   make([]string, 0, modelCount),
		completed: make([]string, 0, modelCount),
	}

	// Override GenerateContentFunc to implement the synchronization
	env.MockClient.GenerateContentFunc = func(ctx context.Context, prompt string) (*gemini.GenerationResult, error) {
		// Extract the model name from the context
		modelName := ctx.Value(modelNameKey).(string)

		// Record that this model started processing
		processOrder.Lock()
		processOrder.started = append(processOrder.started, modelName)
		processOrder.Unlock()

		// Signal that processing has started for this model
		processingStarted.Done()

		// Wait for all models to reach this point (ensures concurrent processing)
		processingBarrier.Wait()

		// Simulate work with a small sleep
		time.Sleep(50 * time.Millisecond)

		// Record completion
		processOrder.Lock()
		processOrder.completed = append(processOrder.completed, modelName)
		processOrder.Unlock()

		// Signal completion
		processingCompleted.Done()

		// Return a result
		return &gemini.GenerationResult{
			Content:      "# Plan Generated by " + modelName + "\n\nThis is a test plan.",
			TokenCount:   1000,
			FinishReason: "STOP",
		}, nil
	}

	// Create a test file
	env.CreateTestFile(t, "src/main.go", `package main

func main() {}`)

	// Create an instructions file
	instructionsFile := env.CreateTestFile(t, "instructions.md", "Test concurrent model processing")

	// Set up the output directory
	outputDir := filepath.Join(env.TestDir, "output")

	// Create a test configuration with multiple models
	modelNames := []string{"model1", "model2", "model3"}
	testConfig := &architect.CliConfig{
		InstructionsFile: instructionsFile,
		OutputDir:        outputDir,
		ModelNames:       modelNames,
		ApiKey:           "test-api-key",
		Paths:            []string{env.TestDir + "/src"},
		LogLevel:         logutil.InfoLevel,
	}

	// Create a custom mock API service that can track model names
	mockAPIService := &mockModelTrackingAPIService{
		logger:     env.Logger,
		mockClient: env.MockClient,
	}

	// Release the barrier after all models have started processing
	go func() {
		// Wait for all models to start
		processingStarted.Wait()
		// Release the barrier to let all continue
		processingBarrier.Done()
	}()

	// Run the application
	ctx := context.Background()
	err := architect.RunInternal(
		ctx,
		testConfig,
		env.Logger,
		mockAPIService,
		env.AuditLogger,
	)

	if err != nil {
		t.Fatalf("RunInternal failed: %v", err)
	}

	// Wait for all models to complete
	processingCompleted.Wait()

	// Verify all models were started and completed
	if len(processOrder.started) != modelCount {
		t.Errorf("Expected %d models to start processing, but got %d", modelCount, len(processOrder.started))
	}

	if len(processOrder.completed) != modelCount {
		t.Errorf("Expected %d models to complete processing, but got %d", modelCount, len(processOrder.completed))
	}

	// Verify that output files were created for all models
	for _, modelName := range modelNames {
		outputFile := filepath.Join(outputDir, modelName+".md")
		if _, err := os.Stat(outputFile); os.IsNotExist(err) {
			t.Errorf("Output file for model %s was not created at %s", modelName, outputFile)
		}
	}
}

// TestConcurrentModelFailureHandling tests that errors are properly aggregated when
// models are processed concurrently
func TestConcurrentModelFailureHandling(t *testing.T) {
	// Set up the test environment
	env := NewTestEnv(t)
	defer env.Cleanup()

	// Use synchronization primitives to track concurrent execution
	var processingStarted sync.WaitGroup
	var processingBarrier sync.WaitGroup

	// Track model execution data
	modelCount := 3
	processingStarted.Add(modelCount)
	processingBarrier.Add(1) // Single barrier to hold all goroutines until all have started

	processedModels := &struct {
		sync.Mutex
		started map[string]bool
		failed  map[string]bool
	}{
		started: make(map[string]bool),
		failed:  make(map[string]bool),
	}

	// Make models 1 and 3 fail with different errors
	env.MockClient.GenerateContentFunc = func(ctx context.Context, prompt string) (*gemini.GenerationResult, error) {
		// Extract the model name from context
		modelName := ctx.Value(modelNameKey).(string)

		// Record that this model started processing
		processedModels.Lock()
		processedModels.started[modelName] = true
		processedModels.Unlock()

		// Signal that processing has started for this model
		processingStarted.Done()

		// Wait for all models to reach this point (ensures concurrent processing)
		processingBarrier.Wait()

		// Simulate work with a small sleep
		time.Sleep(50 * time.Millisecond)

		// Make models 1 and 3 fail with different errors
		if modelName == "model1" {
			processedModels.Lock()
			processedModels.failed[modelName] = true
			processedModels.Unlock()
			return nil, fmt.Errorf("simulated error 1 for model %s", modelName)
		} else if modelName == "model3" {
			processedModels.Lock()
			processedModels.failed[modelName] = true
			processedModels.Unlock()
			return nil, fmt.Errorf("simulated error 3 for model %s", modelName)
		}

		// Return a successful result for model2
		return &gemini.GenerationResult{
			Content:      "# Plan Generated by " + modelName + "\n\nThis is a test plan.",
			TokenCount:   1000,
			FinishReason: "STOP",
		}, nil
	}

	// Create a test file
	env.CreateTestFile(t, "src/main.go", `package main

func main() {}`)

	// Create an instructions file
	instructionsFile := env.CreateTestFile(t, "instructions.md", "Test concurrent model failure handling")

	// Set up the output directory
	outputDir := filepath.Join(env.TestDir, "output")

	// Create a test configuration with multiple models
	modelNames := []string{"model1", "model2", "model3"}
	testConfig := &architect.CliConfig{
		InstructionsFile: instructionsFile,
		OutputDir:        outputDir,
		ModelNames:       modelNames,
		ApiKey:           "test-api-key",
		Paths:            []string{env.TestDir + "/src"},
		LogLevel:         logutil.InfoLevel,
	}

	// Create a custom mock API service that can track model names
	mockAPIService := &mockModelTrackingAPIService{
		logger:     env.Logger,
		mockClient: env.MockClient,
	}

	// Release the barrier after all models have started processing
	go func() {
		// Wait for all models to start
		processingStarted.Wait()
		// Release the barrier to let all continue
		processingBarrier.Done()
	}()

	// Run the application, expecting errors
	ctx := context.Background()
	err := architect.RunInternal(
		ctx,
		testConfig,
		env.Logger,
		mockAPIService,
		env.AuditLogger,
	)

	// Should have errors from model1 and model3
	if err == nil {
		t.Fatalf("Expected RunInternal to return errors, but got nil")
	}

	// Verify that the error contains information about both failed models
	errorMsg := err.Error()
	if !strings.Contains(errorMsg, "model1") {
		t.Errorf("Error message does not mention model1: %v", err)
	}
	if !strings.Contains(errorMsg, "model3") {
		t.Errorf("Error message does not mention model3: %v", err)
	}

	// Verify all models were started
	for _, modelName := range modelNames {
		if !processedModels.started[modelName] {
			t.Errorf("Model %s was not started", modelName)
		}
	}

	// Verify that models 1 and 3 failed
	if !processedModels.failed["model1"] {
		t.Errorf("Expected model1 to fail, but it didn't")
	}
	if !processedModels.failed["model3"] {
		t.Errorf("Expected model3 to fail, but it didn't")
	}
	if processedModels.failed["model2"] {
		t.Errorf("Expected model2 to succeed, but it failed")
	}

	// Verify that output files were created only for successful models
	// Model2 should have an output file
	model2OutputFile := filepath.Join(outputDir, "model2.md")
	if _, err := os.Stat(model2OutputFile); os.IsNotExist(err) {
		t.Errorf("Output file for successful model2 was not created at %s", model2OutputFile)
	}

	// Models 1 and 3 should not have output files since they failed
	model1OutputFile := filepath.Join(outputDir, "model1.md")
	if _, err := os.Stat(model1OutputFile); !os.IsNotExist(err) {
		t.Errorf("Output file for failed model1 was created at %s (it shouldn't exist)", model1OutputFile)
	}

	model3OutputFile := filepath.Join(outputDir, "model3.md")
	if _, err := os.Stat(model3OutputFile); !os.IsNotExist(err) {
		t.Errorf("Output file for failed model3 was created at %s (it shouldn't exist)", model3OutputFile)
	}
}

// TestModelSpecificOutput tests that our implementation only writes model-specific files
// and no longer creates the legacy output.md file
func TestModelSpecificOutput(t *testing.T) {
	// Set up the test environment
	env := NewTestEnv(t)
	defer env.Cleanup()

	// Set up the mock client
	env.SetupMockGeminiClient()

	// Create a test file
	env.CreateTestFile(t, "src/main.go", `package main

func main() {}`)

	// Create an instructions file
	instructionsFile := env.CreateTestFile(t, "instructions.md", "Test model-specific output")

	// Set up the output directory
	outputDir := filepath.Join(env.TestDir, "output")

	// Create a test configuration with a single model
	testConfig := &architect.CliConfig{
		InstructionsFile: instructionsFile,
		OutputDir:        outputDir,
		ModelNames:       []string{"gemini-pro"},
		ApiKey:           "test-api-key",
		Paths:            []string{env.TestDir + "/src"},
		LogLevel:         logutil.InfoLevel,
	}

	// Run the application
	ctx := context.Background()
	err := RunTestWithConfig(ctx, testConfig, env)

	if err != nil {
		t.Fatalf("RunTestWithConfig failed: %v", err)
	}

	// Check that the output directory exists
	if _, err := os.Stat(outputDir); os.IsNotExist(err) {
		t.Errorf("Output directory was not created at %s", outputDir)
	}

	// Check that the model-specific file exists
	modelOutputFile := filepath.Join(outputDir, "gemini-pro.md")
	if _, err := os.Stat(modelOutputFile); os.IsNotExist(err) {
		t.Errorf("Model-specific output file was not created at %s", modelOutputFile)
	}

	// Verify that the legacy output.md file does NOT exist
	legacyOutputFile := filepath.Join(outputDir, "output.md")
	if _, err := os.Stat(legacyOutputFile); !os.IsNotExist(err) {
		t.Errorf("Legacy output.md file still exists at %s but it should not", legacyOutputFile)
	}
}
